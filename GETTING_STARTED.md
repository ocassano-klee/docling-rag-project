# Guide de dÃ©marrage rapide

## ğŸš€ Installation en 5 minutes

### PrÃ©requis

- Python 3.8 ou supÃ©rieur
- Compte AWS avec accÃ¨s Ã  Neptune et OpenSearch
- 2 Go d'espace disque libre

### Ã‰tape 1 : Cloner et installer

```bash
# CrÃ©er l'environnement virtuel
python -m venv venv

# Activer l'environnement
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### Ã‰tape 2 : Configuration

**A. Obtenir une clÃ© API Cohere**

1. CrÃ©er un compte sur [cohere.com](https://cohere.com/)
2. Aller sur [dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys)
3. CrÃ©er une nouvelle clÃ© API

**B. Configurer la clÃ© API**

```bash
# Linux/Mac
export COHERE_API_KEY="votre-cle-api"

# Windows CMD
set COHERE_API_KEY=votre-cle-api

# Windows PowerShell
$env:COHERE_API_KEY="votre-cle-api"
```

**C. Ã‰diter config.yaml avec vos paramÃ¨tres AWS**

```yaml
neptune:
  endpoint: "votre-cluster.neptune.amazonaws.com"
  port: 8182
  region: "eu-west-1"

opensearch:
  endpoint: "https://votre-domaine.es.amazonaws.com"
  index_name: "document-chunks"
  region: "eu-west-1"

embeddings:
  provider: "cohere"
  model: "embed-multilingual-v3"
  dimension: 1024
  api_key: ""  # Laisser vide si variable d'environnement dÃ©finie
```

ğŸ“– **Voir [COHERE_SETUP.md](COHERE_SETUP.md) pour plus de dÃ©tails sur la configuration Cohere**

### Ã‰tape 3 : Test en mode dry-run

```bash
# Placer un PDF dans data/input/
cp votre_document.pdf data/input/

# Tester l'ingestion (sans connexion AWS)
python src/ingestion.py --input data/input/votre_document.pdf --dry-run

# Tester l'interrogation (sans connexion AWS)
python src/query.py --question "Qu'est-ce qu'un Data Fabric?" --dry-run
```

RÃ©sultat : Des fichiers CSV sont gÃ©nÃ©rÃ©s dans `dry_run_output/`

### Ã‰tape 4 : Ingestion rÃ©elle

```bash
# IngÃ©rer le document dans Neptune et OpenSearch
python src/ingestion.py --input data/input/votre_document.pdf
```

Sortie attendue :
```
=== Initialisation du pipeline d'ingestion ===
âœ“ Connexion Neptune Ã©tablie
âœ“ Index document-chunks crÃ©Ã©

Ã‰tape 1/5: Extraction et chunking avec Docling
âœ“ 45 chunks crÃ©Ã©s

Ã‰tape 2/5: GÃ©nÃ©ration des embeddings
âœ“ 45 embeddings gÃ©nÃ©rÃ©s

Ã‰tape 3/5: Insertion des mÃ©tadonnÃ©es dans Neptune
âœ“ 45 chunks insÃ©rÃ©s dans Neptune

Ã‰tape 4/5: Insertion des embeddings dans OpenSearch
âœ“ 45 chunks indexÃ©s dans OpenSearch

âœ“ Traitement terminÃ© avec succÃ¨s
```

### Ã‰tape 5 : Interrogation

```bash
# Poser une question
python src/query.py --question "Quelle est l'architecture du systÃ¨me?"
```

RÃ©sultat : Un fichier texte est crÃ©Ã© dans `data/output/` avec le prompt augmentÃ©

## ğŸ“Š Workflow typique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WORKFLOW COMPLET                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. PRÃ‰PARATION
   â”œâ”€ Configurer config.yaml
   â”œâ”€ Placer PDFs dans data/input/
   â””â”€ VÃ©rifier connexions AWS

2. INGESTION (une fois par document)
   â”œâ”€ python src/ingestion.py --input data/input/doc1.pdf
   â”œâ”€ python src/ingestion.py --input data/input/doc2.pdf
   â””â”€ python src/ingestion.py --input data/input/doc3.pdf

3. INTERROGATION (autant de fois que nÃ©cessaire)
   â”œâ”€ python src/query.py --question "Question 1?"
   â”œâ”€ python src/query.py --question "Question 2?"
   â””â”€ python src/query.py --question "Question 3?"

4. UTILISATION DES PROMPTS
   â”œâ”€ Ouvrir data/output/prompt_*.txt
   â”œâ”€ Copier le contenu
   â””â”€ Envoyer Ã  un LLM (GPT-4, Claude, etc.)
```

## ğŸ¯ Cas d'usage

### Cas 1 : Documentation technique

```bash
# IngÃ©rer la documentation
python src/ingestion.py --input data/input/technical_doc.pdf

# Poser des questions
python src/query.py --question "Comment configurer le systÃ¨me?"
python src/query.py --question "Quelles sont les dÃ©pendances requises?"
python src/query.py --question "Comment rÃ©soudre l'erreur X?"
```

### Cas 2 : Analyse de contrats

```bash
# IngÃ©rer plusieurs contrats
python src/ingestion.py --input data/input/contrat_2024.pdf
python src/ingestion.py --input data/input/contrat_2025.pdf

# Rechercher des clauses spÃ©cifiques
python src/query.py --question "Quelles sont les clauses de rÃ©siliation?"
python src/query.py --question "Quel est le montant total des engagements?"
```

### Cas 3 : Base de connaissances

```bash
# IngÃ©rer toute la documentation
for file in data/input/*.pdf; do
    python src/ingestion.py --input "$file"
done

# Interroger la base
python src/query.py --question "Qu'est-ce qu'un Data Fabric?"
python src/query.py --question "Comment fonctionne l'architecture?"
```

## ğŸ”§ Options avancÃ©es

### Utiliser le filtrage Neptune

```bash
python src/query.py \
    --question "Comment fonctionne l'ingestion?" \
    --use-neptune-filter
```

Avantage : RÃ©duit l'espace de recherche en utilisant le graphe de connaissances

### Personnaliser la configuration

```yaml
# Ajuster les paramÃ¨tres de chunking
docling:
  chunk_size: 1024      # Plus grand = plus de contexte
  chunk_overlap: 100    # Plus grand = meilleure continuitÃ©
  min_chunk_size: 50    # Plus petit = plus de chunks

# Ajuster la recherche
query:
  top_k: 10             # Plus grand = plus de rÃ©sultats
  similarity_threshold: 0.7  # Plus haut = plus strict
```

### Utiliser un meilleur modÃ¨le d'embeddings

Le projet utilise dÃ©jÃ  **Cohere embed-multilingual-v3**, un des meilleurs modÃ¨les pour le franÃ§ais.

Si vous souhaitez utiliser un modÃ¨le local (sans API) :

```yaml
embeddings:
  provider: "sentence-transformers"
  model: "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
  dimension: 768
  batch_size: 32
```

**Note** : NÃ©cessite de rÃ©-ingÃ©rer tous les documents et recrÃ©er l'index OpenSearch.

## ğŸ“ˆ Monitoring

### VÃ©rifier les donnÃ©es dans Neptune

```bash
# Via AWS CLI
aws neptune describe-db-clusters

# Via Neptune Workbench (Gremlin)
g.V().hasLabel('Document').count()
g.V().hasLabel('Chunk').count()
g.V().hasLabel('Annotation').count()
```

### VÃ©rifier l'index OpenSearch

```bash
# Via AWS CLI
aws opensearch describe-domain --domain-name votre-domaine

# Via API
curl -X GET "https://votre-domaine.es.amazonaws.com/document-chunks/_count"
```

## ğŸ› DÃ©pannage rapide

### ProblÃ¨me : "Connection refused" (Neptune)

```bash
# VÃ©rifier l'endpoint
aws neptune describe-db-clusters --query 'DBClusters[*].Endpoint'

# VÃ©rifier les security groups (port 8182 doit Ãªtre ouvert)
```

### ProblÃ¨me : "Index not found" (OpenSearch)

```python
# CrÃ©er l'index manuellement
from opensearch_client import OpenSearchClient
client = OpenSearchClient(...)
client.create_index(dimension=384)
```

### ProblÃ¨me : Chunks de mauvaise qualitÃ©

```yaml
# Ajuster les paramÃ¨tres dans config.yaml
docling:
  chunk_size: 256       # RÃ©duire pour plus de prÃ©cision
  chunk_overlap: 50
  min_chunk_size: 100   # Augmenter pour Ã©viter les petits chunks
```

### ProblÃ¨me : RÃ©sultats non pertinents

```bash
# Augmenter top_k et le seuil
python src/query.py \
    --question "..." \
    --use-neptune-filter  # Activer le filtrage
```

## ğŸ“š Ressources

### Documentation

- [README.md](README.md) - Vue d'ensemble et architecture
- [ARCHITECTURE.md](ARCHITECTURE.md) - DÃ©tails techniques
- [EXAMPLES.md](EXAMPLES.md) - Exemples complets
- [TROUBLESHOOTING.md](TROUBLESHOOTING.md) - Guide de dÃ©pannage

### Diagrammes

- [architecture_diagram.svg](architecture_diagram.svg) - SchÃ©ma visuel

### Scripts

- `quick_start.sh` (Linux/Mac) - Installation automatique
- `quick_start.bat` (Windows) - Installation automatique

## ğŸ’¡ Conseils

### Performance

1. **GPU** : Utiliser un GPU pour les embeddings (10x plus rapide)
2. **Batch** : Traiter plusieurs documents en parallÃ¨le
3. **Cache** : RÃ©utiliser les embeddings quand possible

### QualitÃ©

1. **Chunking** : Ajuster chunk_size selon le type de document
2. **ModÃ¨le** : Utiliser un modÃ¨le adaptÃ© Ã  votre langue
3. **Annotations** : Personnaliser les annotations selon vos besoins

### CoÃ»ts

1. **Neptune** : ArrÃªter le cluster quand non utilisÃ©
2. **OpenSearch** : Utiliser des instances adaptÃ©es Ã  votre charge
3. **Dry-run** : Tester en mode dry-run avant dÃ©ploiement

## ğŸ“ Prochaines Ã©tapes

1. âœ… Installer et tester en mode dry-run
2. âœ… Configurer AWS (Neptune + OpenSearch)
3. âœ… IngÃ©rer vos premiers documents
4. âœ… Tester des questions
5. â¬œ Personnaliser les annotations
6. â¬œ Optimiser les paramÃ¨tres
7. â¬œ IntÃ©grer avec votre LLM prÃ©fÃ©rÃ©
8. â¬œ DÃ©ployer en production

## ğŸ¤ Support

En cas de problÃ¨me :
1. Consulter [TROUBLESHOOTING.md](TROUBLESHOOTING.md)
2. VÃ©rifier les logs
3. Tester en mode dry-run
4. VÃ©rifier la configuration AWS

Bon dÃ©marrage ! ğŸš€
